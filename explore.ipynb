{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/615 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('author', 'affiliated_with', 'institution'), tensor([], device='cuda:0', size=(2, 0), dtype=torch.int64))\n",
      "(('author', 'writes', 'paper'), tensor([[    0,     1,     2,  ..., 27543,  4487,  4489],\n",
      "        [    0,     0,     0,  ...,  7859,  7859,  7859]], device='cuda:0'))\n",
      "(('paper', 'cites', 'paper'), tensor([[ 1024,  1025,  1026,  ..., 50302, 50303, 50304],\n",
      "        [    0,     0,     0,  ...,  7859,  7859,  7859]], device='cuda:0'))\n",
      "(('paper', 'has_topic', 'field_of_study'), tensor([[50305, 50306, 50307,  ..., 80434, 80435, 80436],\n",
      "        [    0,     0,     0,  ...,  3462,  3462,  3462]], device='cuda:0'))\n",
      "(('institution', 'rev_affiliated_with', 'author'), tensor([[   0,    1,    0,  ...,  133,  133,   25],\n",
      "        [   0,    1,    1,  ..., 4488, 4489, 4489]], device='cuda:0'))\n",
      "(('paper', 'rev_writes', 'author'), tensor([[80437, 80438, 80439,  ..., 95202, 50273,  7859],\n",
      "        [    0,     0,     0,  ...,  4489,  4489,  4489]], device='cuda:0'))\n",
      "(('field_of_study', 'rev_has_topic', 'paper'), tensor([[   0,    1,    2,  ..., 4716, 1604, 2704],\n",
      "        [   0,    0,    0,  ..., 7859, 7859, 7859]], device='cuda:0'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m21\u001b[39m):\n\u001b[0;32m---> 93\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     94\u001b[0m     val_acc \u001b[39m=\u001b[39m test(val_loader)\n\u001b[1;32m     95\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m02d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val: \u001b[39m\u001b[39m{\u001b[39;00mval_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m batch_size \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mpaper\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mbatch_size\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39medge_index_dict\u001b[39m.\u001b[39mitems(): \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> 59\u001b[0m stop\n\u001b[1;32m     61\u001b[0m out \u001b[39m=\u001b[39m model(batch\u001b[39m.\u001b[39mx_dict, batch\u001b[39m.\u001b[39medge_index_dict)[\u001b[39m'\u001b[39m\u001b[39mpaper\u001b[39m\u001b[39m'\u001b[39m][:batch_size]\n\u001b[1;32m     62\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(out, batch[\u001b[39m'\u001b[39m\u001b[39mpaper\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39my[:batch_size])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ReLU\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.loader import HGTLoader, NeighborLoader\n",
    "from torch_geometric.nn import Linear, SAGEConv, Sequential, to_hetero\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = T.ToUndirected(merge=True)\n",
    "dataset = OGB_MAG(\"/tmp/\", preprocess='metapath2vec', transform=transform)\n",
    "\n",
    "# Already send node features/labels to GPU for faster access during sampling:\n",
    "data = dataset[0].to(device, 'x', 'y')\n",
    "\n",
    "train_input_nodes = ('paper', data['paper'].train_mask)\n",
    "val_input_nodes = ('paper', data['paper'].val_mask)\n",
    "kwargs = {'batch_size': 1024, 'num_workers': 6, 'persistent_workers': True}\n",
    "\n",
    "train_loader = NeighborLoader(data, num_neighbors=[10] * 2, shuffle=True,\n",
    "                              input_nodes=train_input_nodes, **kwargs)\n",
    "val_loader = NeighborLoader(data, num_neighbors=[10] * 2,\n",
    "                            input_nodes=val_input_nodes, **kwargs)\n",
    "\n",
    "model = Sequential('x, edge_index', [\n",
    "    (SAGEConv((-1, -1), 64), 'x, edge_index -> x'),\n",
    "    ReLU(inplace=True),\n",
    "    (SAGEConv((-1, -1), 64), 'x, edge_index -> x'),\n",
    "    ReLU(inplace=True),\n",
    "    (Linear(-1, dataset.num_classes), 'x -> x'),\n",
    "])\n",
    "model = to_hetero(model, data.metadata(), aggr='sum').to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_params():\n",
    "    # Initialize lazy parameters via forwarding a single batch to the model:\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = batch.to(device, 'edge_index')\n",
    "    model(batch.x_dict, batch.edge_index_dict)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device, 'edge_index')\n",
    "        batch_size = batch['paper'].batch_size\n",
    "\n",
    "        for i in batch.edge_index_dict.items(): print(i)\n",
    "\n",
    "        stop\n",
    "\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)['paper'][:batch_size]\n",
    "        loss = F.cross_entropy(out, batch['paper'].y[:batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_examples += batch_size\n",
    "        total_loss += float(loss) * batch_size\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_examples = total_correct = 0\n",
    "    for batch in tqdm(loader):\n",
    "        batch = batch.to(device, 'edge_index')\n",
    "        batch_size = batch['paper'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)['paper'][:batch_size]\n",
    "        pred = out.argmax(dim=-1)\n",
    "\n",
    "        total_examples += batch_size\n",
    "        total_correct += int((pred == batch['paper'].y[:batch_size]).sum())\n",
    "\n",
    "    return total_correct / total_examples\n",
    "\n",
    "\n",
    "init_params()  # Initialize parameters.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    loss = train()\n",
    "    val_acc = test(val_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
